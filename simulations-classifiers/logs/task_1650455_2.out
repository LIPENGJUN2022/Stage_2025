Running task 2: uv run python src/classifiers/pipeline.py --config config/kernel_size_1/viridiplantae/JTT92_frequencies.json
2025-05-25 14:31:45   Tokenizing real alignments...
2025-05-25 14:31:45   Removing sequences with ambiguous tokens...
2025-05-25 14:31:45   0/20290 sequences have been removed from 0/1497 alignements due to ambiguous sites.
2025-05-25 14:31:45   Tokenizing simulated alignments...
2025-05-25 14:31:46   Removing sequences with ambiguous tokens...
2025-05-25 14:31:46   0/17109 sequences have been removed from 0/2413 alignements due to ambiguous sites.
2025-05-25 14:31:46   Checking alignments...
2025-05-25 14:31:46   30 renamed keys in simulated aligns.
2025-05-25 14:31:46   Creating labels...
2025-05-25 14:31:46   Merging aligns and labels...
2025-05-25 14:31:46   Total number of alignments: 3910
2025-05-25 14:31:46   --- Preprocessing base data ---
2025-05-25 14:31:46   Tokenizing real alignments...
2025-05-25 14:31:46   Removing sequences with ambiguous tokens...
2025-05-25 14:31:46   0/20290 sequences have been removed from 0/1497 alignements due to ambiguous sites.
2025-05-25 14:31:47   Tokenizing simulated alignments...
2025-05-25 14:31:47   Removing sequences with ambiguous tokens...
2025-05-25 14:31:47   0/17109 sequences have been removed from 0/2413 alignements due to ambiguous sites.
2025-05-25 14:31:47   Checking alignments...
2025-05-25 14:31:47   30 renamed keys in simulated aligns.
2025-05-25 14:31:47   Creating labels...
2025-05-25 14:31:47   Merging aligns and labels...
2025-05-25 14:31:47   Total number of alignments: 3910
2025-05-25 14:31:47   --- Start running AACnnClassifier ---
--- Hyperparameters ---
model = AAConvNet(
  (conv_layer): Sequential(
    (0): Conv1d(22, 64, kernel_size=(1,), stride=(1,))
    (1): ReLU()
    (2): AdaptiveAvgPool1d(output_size=1)
    (3): Dropout(p=0.2, inplace=False)
  )
  (dense_layer): Linear(in_features=64, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
kernel_size = 1
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 1537.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.693        0.693       0.410       0.253   0.00000      *
    2        0.666        0.660       0.367       0.000   0.00250      *
    3        0.665        0.659       0.372       0.000   0.00500      *
    4        0.663        0.656       0.472       0.280   0.00750      *
    5        0.659        0.653       0.379       0.032   0.01000      *
    6        0.651        0.643       0.549       0.569   0.00990      *
    7        0.643        0.636       0.579       0.678   0.00980      *
    8        0.630        0.628       0.674       0.774   0.00970      *
    9        0.620        0.610       0.641       0.778   0.00961      *
   10        0.610        0.604       0.674       0.789   0.00951      *
   11        0.599        0.597       0.667       0.787   0.00941      *
   12        0.591        0.592       0.654       0.783   0.00932      *
   13        0.580        0.571       0.697       0.801   0.00923      *
   14        0.574        0.561       0.723       0.814   0.00914      *
   15        0.568        0.565       0.705       0.806   0.00904       
   16        0.561        0.554       0.731       0.819   0.00895      *
   17        0.556        0.552       0.767       0.838   0.00886      *
   18        0.552        0.536       0.777       0.844   0.00878      *
   19        0.547        0.533       0.759       0.834   0.00869      *
   20        0.546        0.532       0.713       0.810   0.00860      *
   21        0.541        0.535       0.777       0.844   0.00851       
   22        0.538        0.524       0.774       0.842   0.00843      *
   23        0.537        0.533       0.772       0.842   0.00835       
   24        0.532        0.520       0.779       0.843   0.00826      *
   25        0.531        0.513       0.764       0.837   0.00818      *
   26        0.528        0.512       0.782       0.846   0.00810      *
   27        0.526        0.516       0.777       0.844   0.00802       
   28        0.523        0.507       0.790       0.848   0.00794      *
   29        0.522        0.499       0.800       0.852   0.00786      *
   30        0.519        0.499       0.785       0.846   0.00778      *
   31        0.518        0.491       0.800       0.851   0.00770      *
   32        0.516        0.494       0.797       0.853   0.00762       
   33        0.515        0.494       0.808       0.859   0.00755       
   34        0.514        0.500       0.787       0.848   0.00747       
   35        0.512        0.492       0.787       0.848   0.00740       
   36        0.510        0.489       0.795       0.852   0.00732      *
   37        0.510        0.482       0.813       0.860   0.00725      *
   38        0.508        0.480       0.810       0.861   0.00718      *
   39        0.508        0.475       0.808       0.858   0.00711      *
   40        0.507        0.485       0.782       0.846   0.00703       
   41        0.507        0.491       0.779       0.844   0.00696       
   42        0.505        0.481       0.800       0.855   0.00689       
   43        0.505        0.490       0.785       0.847   0.00683       
   44        0.503        0.475       0.813       0.863   0.00676       
   45        0.502        0.476       0.813       0.863   0.00669       
   46        0.504        0.491       0.777       0.842   0.00662       
   47        0.501        0.478       0.805       0.859   0.00656       
   48        0.500        0.474       0.810       0.861   0.00649      *
   49        0.500        0.475       0.808       0.859   0.00643       
   50        0.499        0.471       0.810       0.861   0.00636      *
   51        0.499        0.481       0.810       0.862   0.00630       
   52        0.498        0.476       0.805       0.857   0.00624       
   53        0.498        0.473       0.805       0.857   0.00617       
   54        0.499        0.483       0.797       0.854   0.00611       
   55        0.498        0.477       0.813       0.864   0.00605       
   56        0.497        0.466       0.810       0.858   0.00599      *
   57        0.497        0.464       0.808       0.855   0.00593      *
   58        0.496        0.473       0.813       0.864   0.00587       
   59        0.495        0.463       0.813       0.861   0.00581      *
   60        0.495        0.470       0.808       0.860   0.00575       
   61        0.495        0.471       0.810       0.862   0.00570       
   62        0.495        0.471       0.810       0.862   0.00564       
   63        0.494        0.472       0.810       0.862   0.00558       
   64        0.493        0.467       0.808       0.858   0.00553       
   65        0.493        0.469       0.813       0.863   0.00547       
   66        0.493        0.468       0.810       0.862   0.00542       
   67        0.493        0.472       0.810       0.862   0.00536       
   68        0.492        0.467       0.813       0.863   0.00531       
   69        0.493        0.472       0.810       0.861   0.00526       
   70        0.491        0.462       0.805       0.856   0.00520      *
   71        0.491        0.465       0.813       0.863   0.00515       
   72        0.491        0.461       0.805       0.856   0.00510      *
   73        0.492        0.473       0.808       0.860   0.00505       
   74        0.491        0.457       0.815       0.861   0.00500      *
   75        0.490        0.458       0.813       0.860   0.00495       
   76        0.490        0.456       0.815       0.862   0.00490      *
   77        0.490        0.455       0.815       0.863   0.00485      *
   78        0.490        0.453       0.815       0.862   0.00480      *
   79        0.490        0.459       0.813       0.863   0.00475       
   80        0.489        0.460       0.810       0.861   0.00471       
   81        0.489        0.463       0.805       0.856   0.00466       
   82        0.489        0.460       0.808       0.858   0.00461       
   83        0.489        0.462       0.805       0.856   0.00457       
   84        0.489        0.450       0.810       0.855   0.00452      *
   85        0.488        0.458       0.813       0.863   0.00448       
   86        0.488        0.454       0.813       0.861   0.00443       
   87        0.487        0.453       0.821       0.867   0.00439       
   88        0.488        0.448       0.815       0.860   0.00434      *
   89        0.488        0.462       0.813       0.863   0.00430       
   90        0.488        0.461       0.813       0.863   0.00426       
   91        0.488        0.461       0.808       0.858   0.00421       
   92        0.488        0.462       0.810       0.860   0.00417       
   93        0.487        0.455       0.813       0.860   0.00413       
   94        0.487        0.452       0.818       0.864   0.00409       
   95        0.486        0.452       0.810       0.859   0.00405       
   96        0.486        0.452       0.813       0.861   0.00401       
   97        0.486        0.456       0.815       0.864   0.00397       
   98        0.486        0.450       0.813       0.861   0.00393       
   99        0.485        0.452       0.810       0.859   0.00389       
  100        0.485        0.447       0.815       0.863   0.00385      *
  101        0.485        0.448       0.815       0.863   0.00381       
  102        0.485        0.448       0.815       0.863   0.00377       
  103        0.485        0.451       0.815       0.864   0.00373       
  104        0.485        0.450       0.810       0.859   0.00370       
  105        0.485        0.449       0.818       0.864   0.00366       
  106        0.485        0.449       0.823       0.868   0.00362       
  107        0.486        0.464       0.810       0.861   0.00359       
  108        0.485        0.454       0.810       0.859   0.00355       
  109        0.485        0.450       0.821       0.866   0.00352       
  110        0.484        0.454       0.813       0.861   0.00348       
  111        0.484        0.447       0.821       0.864   0.00345       
  112        0.484        0.446       0.823       0.868   0.00341      *
  113        0.484        0.453       0.813       0.862   0.00338       
  114        0.484        0.450       0.815       0.863   0.00334       
  115        0.484        0.452       0.808       0.858   0.00331       
  116        0.483        0.447       0.818       0.864   0.00328       
  117        0.484        0.440       0.815       0.859   0.00324      *
  118        0.483        0.447       0.818       0.865   0.00321       
  119        0.483        0.445       0.815       0.863   0.00318       
  120        0.483        0.450       0.810       0.860   0.00315       
  121        0.483        0.451       0.808       0.858   0.00312       
  122        0.483        0.445       0.823       0.867   0.00309       
  123        0.485        0.463       0.808       0.859   0.00305       
  124        0.483        0.453       0.813       0.861   0.00302       
  125        0.483        0.451       0.810       0.859   0.00299       
  126        0.483        0.444       0.813       0.858   0.00296       
  127        0.483        0.448       0.818       0.864   0.00293       
  128        0.483        0.452       0.810       0.860   0.00290       
  129        0.482        0.450       0.810       0.859   0.00288       
  130        0.483        0.456       0.808       0.858   0.00285       
  131        0.482        0.447       0.818       0.864   0.00282       
  132        0.482        0.452       0.813       0.861   0.00279       
  133        0.482        0.447       0.821       0.866   0.00276       
  134        0.483        0.457       0.810       0.860   0.00273       
  135        0.482        0.447       0.815       0.863   0.00271       
  136        0.482        0.450       0.810       0.859   0.00268       
  137        0.481        0.448       0.813       0.861   0.00265       
--- Training ended ---
Number of model parameters: 1537
Training time: 0:03:11
Best epoch: 117
Best valid loss: 0.440
Best valid accuracy (macro): 0.815
Best F1 score: 0.859
2025-05-25 14:35:00   --- End running AACnnClassifier ---
