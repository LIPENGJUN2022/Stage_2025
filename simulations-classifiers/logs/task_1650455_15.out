Running task 15: uv run python src/classifiers/pipeline.py --config config/split_seq/comparsion_mammals_viridiplantae/mammals_viridiplantae.json
2025-05-25 15:13:49   Tokenizing real alignments...
2025-05-25 15:13:50   Removing sequences with ambiguous tokens...
2025-05-25 15:13:50   0/20290 sequences have been removed from 0/20290 alignements due to ambiguous sites.
2025-05-25 15:13:50   Tokenizing simulated alignments...
2025-05-25 15:13:51   Removing sequences with ambiguous tokens...
2025-05-25 15:13:52   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 15:13:52   Checking alignments...
2025-05-25 15:13:52   Creating labels...
2025-05-25 15:13:52   Merging aligns and labels...
2025-05-25 15:13:52   Total number of alignments: 37399
2025-05-25 15:13:52   --- Preprocessing base data ---
2025-05-25 15:13:52   Tokenizing real alignments...
2025-05-25 15:13:53   Removing sequences with ambiguous tokens...
2025-05-25 15:13:53   0/20290 sequences have been removed from 0/20290 alignements due to ambiguous sites.
2025-05-25 15:13:53   Tokenizing simulated alignments...
2025-05-25 15:13:54   Removing sequences with ambiguous tokens...
2025-05-25 15:13:54   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 15:13:54   Checking alignments...
2025-05-25 15:13:54   Creating labels...
2025-05-25 15:13:54   Merging aligns and labels...
2025-05-25 15:13:54   Total number of alignments: 37399
2025-05-25 15:13:54   --- Start running LogisticRegressionClassifier ---
2025-05-25 15:13:54   --- Preprocessing data ---
2025-05-25 15:13:54   --- Start training ---
2025-05-25 15:13:54   Number of cross validation folds: 50
2025-05-25 15:13:54   Shuffle data: True
2025-05-25 15:13:54   Scale features: True
2025-05-25 15:13:56   --- Training ended ---
2025-05-25 15:13:56   Training time: 0:00:02
2025-05-25 15:13:56   Fold F1 scores: ('0.7725', '0.7847', '0.7679', '0.7699', '0.7681', '0.7304', '0.7318', '0.7719', '0.7515', '0.7835', '0.7534', '0.7426', '0.7778', '0.7365', '0.7545', '0.7541', '0.7736', '0.723', '0.7256', '0.7608', '0.756', '0.7301', '0.7336', '0.7388', '0.7477', '0.7434', '0.7685', '0.7848', '0.7216', '0.7626', '0.7881', '0.7448', '0.7738', '0.7722', '0.7326', '0.7813', '0.7842', '0.7385', '0.7367', '0.7454', '0.7317', '0.7248', '0.7359', '0.7182', '0.758', '0.7766', '0.7881', '0.7699', '0.7748', '0.7589')
2025-05-25 15:13:56   Fold accuracies: ('0.7968', '0.8115', '0.7914', '0.7995', '0.7941', '0.766', '0.7687', '0.7914', '0.7807', '0.8102', '0.7821', '0.7674', '0.8021', '0.7714', '0.7807', '0.7794', '0.7981', '0.7634', '0.7634', '0.7781', '0.7834', '0.7647', '0.766', '0.7741', '0.7807', '0.7794', '0.7914', '0.8102', '0.7607', '0.7928', '0.8102', '0.7701', '0.7968', '0.7981', '0.7687', '0.7995', '0.8102', '0.7727', '0.7687', '0.7781', '0.7647', '0.7553', '0.762', '0.766', '0.7874', '0.8008', '0.8102', '0.7995', '0.7995', '0.7831')
2025-05-25 15:13:56   
2025-05-25 15:13:56   --- End running LogisticRegressionClassifier ---
2025-05-25 15:13:56   --- Start running DenseMsaClassifier ---
--- Hyperparameters ---
model = DenseMsaNet(
  (dense_layer1): Sequential(
    (0): Linear(in_features=22, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=False)
  )
  (dense_layer2): Linear(in_features=100, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 2401.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.693        0.693       0.539       0.001   0.00000      *
    2        0.464        0.463       0.789       0.769   0.00250      *
    3        0.446        0.446       0.792       0.762   0.00500      *
    4        0.438        0.440       0.790       0.747   0.00750      *
    5        0.413        0.414       0.805       0.780   0.01000      *
    6        0.409        0.410       0.810       0.774   0.00990      *
    7        0.401        0.403       0.814       0.797   0.00980      *
    8        0.404        0.406       0.819       0.804   0.00970       
    9        0.403        0.405       0.810       0.803   0.00961       
   10        0.379        0.383       0.826       0.802   0.00951      *
   11        0.378        0.383       0.826       0.795   0.00941      *
   12        0.375        0.382       0.833       0.818   0.00932      *
   13        0.372        0.377       0.829       0.810   0.00923      *
   14        0.367        0.374       0.830       0.803   0.00914      *
   15        0.364        0.371       0.839       0.815   0.00904      *
   16        0.363        0.369       0.835       0.812   0.00895      *
   17        0.362        0.370       0.838       0.815   0.00886       
   18        0.362        0.371       0.837       0.811   0.00878       
   19        0.369        0.377       0.831       0.821   0.00869       
   20        0.362        0.372       0.829       0.799   0.00860       
   21        0.355        0.365       0.837       0.813   0.00851      *
   22        0.353        0.362       0.844       0.825   0.00843      *
   23        0.350        0.360       0.841       0.821   0.00835      *
   24        0.355        0.364       0.836       0.822   0.00826       
   25        0.351        0.361       0.845       0.826   0.00818       
   26        0.345        0.355       0.849       0.830   0.00810      *
   27        0.346        0.358       0.845       0.825   0.00802       
   28        0.345        0.357       0.842       0.817   0.00794       
   29        0.343        0.355       0.850       0.827   0.00786       
   30        0.356        0.368       0.831       0.797   0.00778       
   31        0.358        0.367       0.842       0.837   0.00770       
   32        0.342        0.354       0.846       0.826   0.00762      *
   33        0.344        0.353       0.844       0.827   0.00755      *
   34        0.347        0.357       0.849       0.838   0.00747       
   35        0.337        0.348       0.851       0.832   0.00740      *
   36        0.341        0.349       0.847       0.833   0.00732       
   37        0.338        0.349       0.851       0.834   0.00725       
   38        0.338        0.349       0.848       0.829   0.00718       
   39        0.339        0.349       0.854       0.837   0.00711       
   40        0.337        0.349       0.847       0.829   0.00703       
   41        0.335        0.346       0.850       0.832   0.00696      *
   42        0.334        0.347       0.847       0.825   0.00689       
   43        0.340        0.351       0.855       0.844   0.00683       
   44        0.334        0.347       0.854       0.835   0.00676       
   45        0.333        0.346       0.854       0.834   0.00669      *
   46        0.337        0.349       0.844       0.819   0.00662       
   47        0.334        0.344       0.853       0.840   0.00656      *
   48        0.333        0.343       0.856       0.838   0.00649      *
   49        0.333        0.345       0.850       0.824   0.00643       
   50        0.337        0.348       0.854       0.842   0.00636       
   51        0.331        0.343       0.848       0.825   0.00630      *
   52        0.334        0.345       0.851       0.836   0.00624       
   53        0.337        0.351       0.839       0.810   0.00617       
   54        0.329        0.342       0.854       0.834   0.00611      *
   55        0.333        0.344       0.846       0.821   0.00605       
   56        0.338        0.351       0.846       0.818   0.00599       
   57        0.328        0.340       0.856       0.839   0.00593      *
   58        0.338        0.350       0.845       0.816   0.00587       
   59        0.328        0.338       0.853       0.835   0.00581      *
   60        0.326        0.340       0.854       0.835   0.00575       
   61        0.326        0.342       0.855       0.835   0.00570       
   62        0.330        0.341       0.858       0.844   0.00564       
   63        0.325        0.338       0.860       0.843   0.00558       
   64        0.324        0.336       0.864       0.849   0.00553      *
   65        0.331        0.344       0.849       0.824   0.00547       
   66        0.326        0.338       0.859       0.844   0.00542       
   67        0.325        0.339       0.858       0.839   0.00536       
   68        0.323        0.337       0.858       0.840   0.00531       
   69        0.324        0.338       0.860       0.842   0.00526       
   70        0.322        0.336       0.856       0.837   0.00520       
   71        0.325        0.339       0.855       0.834   0.00515       
   72        0.328        0.341       0.848       0.823   0.00510       
   73        0.338        0.346       0.852       0.845   0.00505       
   74        0.325        0.337       0.857       0.839   0.00500       
   75        0.329        0.341       0.848       0.821   0.00495       
   76        0.322        0.334       0.856       0.835   0.00490      *
   77        0.322        0.334       0.857       0.835   0.00485       
   78        0.324        0.338       0.856       0.835   0.00480       
   79        0.327        0.341       0.856       0.836   0.00475       
   80        0.321        0.334       0.859       0.841   0.00471       
   81        0.321        0.335       0.856       0.836   0.00466       
   82        0.322        0.335       0.856       0.835   0.00461       
   83        0.320        0.332       0.859       0.844   0.00457      *
   84        0.321        0.332       0.860       0.845   0.00452       
   85        0.327        0.341       0.846       0.820   0.00448       
   86        0.319        0.332       0.866       0.851   0.00443       
   87        0.323        0.338       0.854       0.829   0.00439       
   88        0.322        0.335       0.854       0.831   0.00434       
   89        0.327        0.343       0.850       0.823   0.00430       
   90        0.318        0.331       0.862       0.844   0.00426      *
   91        0.324        0.336       0.848       0.823   0.00421       
   92        0.319        0.332       0.861       0.846   0.00417       
   93        0.319        0.334       0.853       0.830   0.00413       
   94        0.319        0.332       0.858       0.839   0.00409       
   95        0.318        0.330       0.862       0.843   0.00405      *
   96        0.315        0.328       0.864       0.848   0.00401      *
   97        0.318        0.333       0.863       0.847   0.00397       
   98        0.316        0.329       0.861       0.844   0.00393       
   99        0.316        0.329       0.864       0.848   0.00389       
  100        0.316        0.328       0.864       0.849   0.00385       
  101        0.317        0.328       0.861       0.845   0.00381       
  102        0.315        0.330       0.859       0.841   0.00377       
  103        0.314        0.328       0.860       0.840   0.00373       
  104        0.315        0.327       0.857       0.840   0.00370      *
  105        0.321        0.333       0.863       0.851   0.00366       
  106        0.314        0.329       0.860       0.842   0.00362       
  107        0.320        0.330       0.857       0.845   0.00359       
  108        0.316        0.328       0.861       0.842   0.00355       
  109        0.315        0.328       0.865       0.851   0.00352       
  110        0.314        0.328       0.863       0.845   0.00348       
  111        0.319        0.332       0.860       0.837   0.00345       
  112        0.313        0.327       0.867       0.851   0.00341      *
  113        0.316        0.330       0.861       0.842   0.00338       
  114        0.318        0.330       0.862       0.842   0.00334       
  115        0.315        0.327       0.861       0.842   0.00331       
  116        0.316        0.329       0.857       0.835   0.00328       
  117        0.315        0.329       0.861       0.844   0.00324       
  118        0.314        0.327       0.862       0.843   0.00321       
  119        0.314        0.327       0.865       0.852   0.00318       
  120        0.313        0.327       0.865       0.848   0.00315       
  121        0.311        0.326       0.862       0.846   0.00312      *
  122        0.314        0.327       0.865       0.849   0.00309       
  123        0.314        0.326       0.859       0.840   0.00305       
  124        0.312        0.327       0.862       0.846   0.00302       
  125        0.314        0.329       0.866       0.850   0.00299       
  126        0.313        0.325       0.863       0.846   0.00296      *
  127        0.314        0.328       0.863       0.842   0.00293       
  128        0.313        0.325       0.864       0.851   0.00290       
  129        0.312        0.326       0.861       0.843   0.00288       
  130        0.314        0.327       0.863       0.846   0.00285       
  131        0.313        0.328       0.860       0.840   0.00282       
  132        0.314        0.328       0.865       0.850   0.00279       
  133        0.311        0.326       0.865       0.850   0.00276       
  134        0.311        0.325       0.861       0.844   0.00273      *
  135        0.313        0.328       0.865       0.851   0.00271       
  136        0.311        0.326       0.862       0.845   0.00268       
  137        0.313        0.328       0.859       0.837   0.00265       
  138        0.312        0.326       0.863       0.843   0.00263       
  139        0.320        0.332       0.861       0.852   0.00260       
  140        0.316        0.330       0.863       0.851   0.00257       
  141        0.312        0.326       0.868       0.853   0.00255       
  142        0.315        0.329       0.859       0.836   0.00252       
  143        0.315        0.328       0.859       0.838   0.00250       
  144        0.311        0.325       0.867       0.850   0.00247       
  145        0.313        0.327       0.862       0.841   0.00245       
  146        0.320        0.334       0.855       0.830   0.00242       
  147        0.312        0.328       0.859       0.837   0.00240       
  148        0.313        0.325       0.860       0.841   0.00238       
  149        0.311        0.324       0.863       0.849   0.00235      *
  150        0.310        0.324       0.865       0.849   0.00233      *
  151        0.311        0.325       0.864       0.847   0.00231       
  152        0.311        0.326       0.859       0.839   0.00228       
  153        0.313        0.327       0.858       0.836   0.00226       
  154        0.310        0.325       0.863       0.846   0.00224       
  155        0.311        0.326       0.860       0.839   0.00221       
  156        0.310        0.326       0.864       0.844   0.00219       
  157        0.309        0.325       0.865       0.849   0.00217       
  158        0.310        0.324       0.864       0.849   0.00215       
  159        0.312        0.326       0.867       0.854   0.00213       
  160        0.310        0.325       0.864       0.849   0.00211       
  161        0.311        0.326       0.861       0.838   0.00208       
  162        0.309        0.324       0.866       0.850   0.00206      *
  163        0.310        0.324       0.864       0.847   0.00204       
  164        0.312        0.326       0.863       0.842   0.00202       
  165        0.309        0.324       0.866       0.849   0.00200       
  166        0.311        0.327       0.858       0.835   0.00198       
  167        0.309        0.324       0.864       0.846   0.00196      *
  168        0.310        0.325       0.863       0.848   0.00194       
  169        0.310        0.325       0.866       0.850   0.00192       
  170        0.319        0.334       0.852       0.825   0.00190       
  171        0.309        0.323       0.868       0.853   0.00189      *
  172        0.310        0.326       0.858       0.838   0.00187       
  173        0.310        0.325       0.863       0.843   0.00185       
  174        0.315        0.327       0.866       0.855   0.00183       
  175        0.310        0.324       0.863       0.844   0.00181       
  176        0.309        0.322       0.864       0.848   0.00179      *
  177        0.309        0.323       0.864       0.848   0.00178       
  178        0.309        0.322       0.864       0.846   0.00176       
  179        0.307        0.322       0.863       0.844   0.00174       
  180        0.308        0.321       0.866       0.850   0.00172      *
  181        0.307        0.322       0.868       0.853   0.00171       
  182        0.311        0.326       0.864       0.847   0.00169       
  183        0.309        0.323       0.864       0.846   0.00167       
  184        0.306        0.321       0.867       0.852   0.00165      *
  185        0.308        0.322       0.866       0.853   0.00164       
  186        0.307        0.322       0.868       0.854   0.00162       
  187        0.309        0.325       0.863       0.843   0.00161       
  188        0.307        0.322       0.865       0.849   0.00159       
  189        0.307        0.322       0.863       0.844   0.00157       
  190        0.307        0.321       0.865       0.848   0.00156       
  191        0.306        0.320       0.869       0.853   0.00154      *
  192        0.315        0.331       0.851       0.824   0.00153       
  193        0.307        0.322       0.866       0.848   0.00151       
  194        0.310        0.323       0.866       0.851   0.00150       
  195        0.307        0.321       0.867       0.851   0.00148       
  196        0.307        0.321       0.867       0.852   0.00147       
  197        0.309        0.324       0.865       0.851   0.00145       
  198        0.308        0.322       0.864       0.845   0.00144       
  199        0.307        0.321       0.865       0.847   0.00142       
  200        0.307        0.322       0.865       0.851   0.00141       
  201        0.306        0.321       0.863       0.846   0.00139       
  202        0.306        0.320       0.868       0.853   0.00138      *
  203        0.307        0.321       0.865       0.847   0.00137       
  204        0.308        0.321       0.868       0.852   0.00135       
  205        0.309        0.324       0.863       0.842   0.00134       
  206        0.311        0.325       0.865       0.854   0.00133       
  207        0.307        0.322       0.865       0.849   0.00131       
  208        0.308        0.324       0.863       0.843   0.00130       
  209        0.306        0.321       0.867       0.850   0.00129       
  210        0.306        0.320       0.867       0.852   0.00127       
  211        0.306        0.320       0.866       0.849   0.00126       
  212        0.309        0.322       0.871       0.858   0.00125       
  213        0.307        0.322       0.867       0.852   0.00124       
  214        0.306        0.321       0.868       0.853   0.00122       
  215        0.307        0.321       0.865       0.847   0.00121       
  216        0.306        0.321       0.866       0.849   0.00120       
  217        0.307        0.322       0.862       0.842   0.00119       
  218        0.309        0.323       0.866       0.854   0.00118       
  219        0.307        0.321       0.864       0.848   0.00116       
  220        0.306        0.322       0.867       0.850   0.00115       
  221        0.307        0.322       0.866       0.847   0.00114       
  222        0.306        0.321       0.867       0.849   0.00113       
--- Training ended ---
Number of model parameters: 2401
Training time: 0:11:02
Best epoch: 202
Best valid loss: 0.320
Best valid accuracy (macro): 0.868
Best F1 score: 0.853
2025-05-25 15:24:59   --- End running DenseMsaClassifier ---
2025-05-25 15:24:59   --- Start running DenseSiteClassifier ---
--- Hyperparameters ---
model = DenseSiteNet(
  (dense_layer1): Sequential(
    (0): Linear(in_features=106722, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=False)
  )
  (dense_layer2): Linear(in_features=100, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 10672401.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.694        0.695       0.534       0.236   0.00000      *
    2        0.024        0.045       0.987       0.986   0.00250      *
    3        0.010        0.042       0.986       0.985   0.00500      *
    4        0.009        0.041       0.987       0.985   0.00750      *
    5        0.009        0.031       0.990       0.989   0.01000      *
    6        0.001        0.018       0.995       0.994   0.00990      *
    7        0.002        0.025       0.993       0.993   0.00980       
    8        0.003        0.028       0.994       0.993   0.00970       
    9        0.003        0.034       0.992       0.992   0.00961       
   10        0.001        0.029       0.994       0.994   0.00951       
   11        0.001        0.031       0.993       0.992   0.00941       
   12        0.001        0.030       0.993       0.993   0.00932       
   13        0.002        0.039       0.992       0.991   0.00923       
   14        0.001        0.026       0.994       0.994   0.00914       
   15        0.001        0.043       0.992       0.991   0.00904       
   16        0.000        0.033       0.993       0.992   0.00895       
   17        0.000        0.030       0.993       0.992   0.00886       
   18        0.000        0.027       0.995       0.994   0.00878       
   19        0.001        0.022       0.995       0.994   0.00869       
   20        0.000        0.031       0.995       0.994   0.00860       
   21        0.001        0.041       0.993       0.993   0.00851       
   22        0.001        0.038       0.993       0.992   0.00843       
   23        0.000        0.041       0.994       0.994   0.00835       
   24        0.001        0.044       0.994       0.993   0.00826       
   25        0.000        0.030       0.993       0.992   0.00818       
   26        0.000        0.034       0.994       0.994   0.00810       
--- Training ended ---
Number of model parameters: 10672401
Training time: 0:05:03
Best epoch: 6
Best valid loss: 0.018
Best valid accuracy (macro): 0.995
Best F1 score: 0.994
2025-05-25 15:30:15   --- End running DenseSiteClassifier ---
2025-05-25 15:30:15   --- Start running AACnnClassifier ---
--- Hyperparameters ---
model = AAConvNet(
  (conv_layer): Sequential(
    (0): Conv1d(22, 64, kernel_size=(1,), stride=(1,))
    (1): ReLU()
    (2): AdaptiveAvgPool1d(output_size=1)
    (3): Dropout(p=0.2, inplace=False)
  )
  (dense_layer): Linear(in_features=64, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
kernel_size = 1
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 1537.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.698        0.698       0.542       0.000   0.00000      *
    2        0.681        0.681       0.543       0.006   0.00250      *
    3        0.624        0.622       0.722       0.606   0.00500      *
    4        0.564        0.560       0.766       0.703   0.00750      *
    5        0.528        0.522       0.783       0.742   0.01000      *
    6        0.520        0.512       0.779       0.765   0.00990      *
    7        0.503        0.495       0.792       0.747   0.00980      *
    8        0.498        0.489       0.795       0.756   0.00970      *
    9        0.497        0.490       0.796       0.750   0.00961       
   10        0.494        0.485       0.796       0.758   0.00951      *
   11        0.494        0.486       0.797       0.754   0.00941       
   12        0.495        0.487       0.796       0.754   0.00932       
   13        0.494        0.486       0.793       0.746   0.00923       
   14        0.493        0.484       0.798       0.755   0.00914      *
   15        0.492        0.484       0.794       0.748   0.00904       
   16        0.493        0.483       0.801       0.768   0.00895      *
   17        0.493        0.485       0.798       0.755   0.00886       
   18        0.492        0.483       0.798       0.758   0.00878       
   19        0.492        0.482       0.800       0.765   0.00869      *
   20        0.493        0.482       0.799       0.768   0.00860      *
   21        0.493        0.485       0.791       0.740   0.00851       
   22        0.493        0.486       0.795       0.750   0.00843       
   23        0.493        0.483       0.799       0.767   0.00835       
   24        0.493        0.484       0.796       0.757   0.00826       
   25        0.493        0.483       0.799       0.761   0.00818       
   26        0.494        0.483       0.800       0.768   0.00810       
   27        0.493        0.485       0.793       0.747   0.00802       
   28        0.492        0.483       0.798       0.755   0.00794       
   29        0.492        0.483       0.797       0.754   0.00786       
   30        0.492        0.483       0.796       0.757   0.00778       
   31        0.491        0.482       0.797       0.760   0.00770      *
   32        0.491        0.482       0.795       0.752   0.00762       
   33        0.492        0.482       0.798       0.765   0.00755       
   34        0.491        0.482       0.796       0.757   0.00747      *
   35        0.491        0.482       0.798       0.758   0.00740       
   36        0.492        0.484       0.796       0.752   0.00732       
   37        0.492        0.482       0.797       0.759   0.00725       
   38        0.491        0.483       0.797       0.755   0.00718       
   39        0.492        0.482       0.799       0.765   0.00711       
   40        0.491        0.482       0.798       0.759   0.00703       
   41        0.492        0.482       0.797       0.759   0.00696       
   42        0.492        0.483       0.797       0.755   0.00689       
   43        0.492        0.483       0.798       0.757   0.00683       
   44        0.492        0.484       0.796       0.754   0.00676       
   45        0.492        0.482       0.797       0.760   0.00669       
   46        0.492        0.483       0.797       0.758   0.00662       
   47        0.492        0.483       0.797       0.757   0.00656       
   48        0.492        0.483       0.796       0.757   0.00649       
   49        0.492        0.483       0.797       0.755   0.00643       
   50        0.492        0.482       0.797       0.757   0.00636       
   51        0.492        0.483       0.794       0.750   0.00630       
   52        0.494        0.486       0.788       0.733   0.00624       
   53        0.492        0.483       0.798       0.762   0.00617       
   54        0.493        0.482       0.798       0.765   0.00611       
--- Training ended ---
Number of model parameters: 1537
Training time: 0:13:55
Best epoch: 34
Best valid loss: 0.482
Best valid accuracy (macro): 0.796
Best F1 score: 0.757
2025-05-25 15:44:23   --- End running AACnnClassifier ---
