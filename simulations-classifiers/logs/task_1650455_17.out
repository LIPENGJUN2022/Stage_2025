Running task 17: uv run python src/classifiers/pipeline.py --config config/split_seq/viridiplantae/JTT92_frequencies.json
2025-05-25 16:09:31   Tokenizing real alignments...
2025-05-25 16:09:32   Removing sequences with ambiguous tokens...
2025-05-25 16:09:33   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 16:09:33   Tokenizing simulated alignments...
2025-05-25 16:09:34   Removing sequences with ambiguous tokens...
2025-05-25 16:09:34   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 16:09:34   Checking alignments...
2025-05-25 16:09:34   17109 renamed keys in simulated aligns.
2025-05-25 16:09:34   Creating labels...
2025-05-25 16:09:34   Merging aligns and labels...
2025-05-25 16:09:34   Total number of alignments: 34218
2025-05-25 16:09:34   --- Preprocessing base data ---
2025-05-25 16:09:34   Tokenizing real alignments...
2025-05-25 16:09:35   Removing sequences with ambiguous tokens...
2025-05-25 16:09:35   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 16:09:35   Tokenizing simulated alignments...
2025-05-25 16:09:36   Removing sequences with ambiguous tokens...
2025-05-25 16:09:36   0/17109 sequences have been removed from 0/17109 alignements due to ambiguous sites.
2025-05-25 16:09:36   Checking alignments...
2025-05-25 16:09:37   17109 renamed keys in simulated aligns.
2025-05-25 16:09:37   Creating labels...
2025-05-25 16:09:37   Merging aligns and labels...
2025-05-25 16:09:37   Total number of alignments: 34218
2025-05-25 16:09:37   --- Start running LogisticRegressionClassifier ---
2025-05-25 16:09:37   --- Preprocessing data ---
2025-05-25 16:09:37   --- Start training ---
2025-05-25 16:09:37   Number of cross validation folds: 50
2025-05-25 16:09:37   Shuffle data: True
2025-05-25 16:09:37   Scale features: True
2025-05-25 16:09:38   --- Training ended ---
2025-05-25 16:09:38   Training time: 0:00:01
2025-05-25 16:09:38   Fold F1 scores: ('0.5104', '0.47', '0.5022', '0.4914', '0.5118', '0.4903', '0.455', '0.5', '0.5066', '0.4695', '0.4604', '0.5167', '0.5073', '0.5015', '0.4557', '0.4855', '0.5156', '0.4727', '0.4758', '0.5066', '0.4871', '0.503', '0.5022', '0.4713', '0.4701', '0.4481', '0.5269', '0.5231', '0.4926', '0.491', '0.5037', '0.4949', '0.4985', '0.5202', '0.514', '0.4841', '0.5145', '0.5014', '0.4882', '0.4772', '0.4662', '0.5269', '0.5015', '0.5427', '0.4606', '0.4949', '0.4978', '0.4615', '0.5174', '0.4948')
2025-05-25 16:09:38   Fold accuracies: ('0.5212', '0.4964', '0.5109', '0.4832', '0.5153', '0.5022', '0.4788', '0.5036', '0.5109', '0.4788', '0.473', '0.5139', '0.5095', '0.5182', '0.4803', '0.4803', '0.5226', '0.492', '0.4781', '0.5044', '0.5044', '0.5205', '0.5015', '0.4751', '0.4825', '0.4635', '0.538', '0.5175', '0.4971', '0.5029', '0.5132', '0.4956', '0.5146', '0.5307', '0.5161', '0.4795', '0.5336', '0.4942', '0.4912', '0.481', '0.481', '0.5249', '0.5088', '0.5541', '0.4898', '0.4956', '0.5073', '0.4985', '0.5146', '0.4985')
2025-05-25 16:09:38   
2025-05-25 16:09:38   --- End running LogisticRegressionClassifier ---
2025-05-25 16:09:38   --- Start running DenseMsaClassifier ---
--- Hyperparameters ---
model = DenseMsaNet(
  (dense_layer1): Sequential(
    (0): Linear(in_features=22, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=False)
  )
  (dense_layer2): Linear(in_features=100, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 2401.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.693        0.694       0.495       0.022   0.00000      *
    2        0.690        0.690       0.560       0.617   0.00250      *
    3        0.682        0.682       0.582       0.424   0.00500      *
    4        0.665        0.662       0.628       0.617   0.00750      *
    5        0.651        0.650       0.627       0.602   0.01000      *
    6        0.642        0.641       0.643       0.616   0.00990      *
    7        0.638        0.635       0.654       0.647   0.00980      *
    8        0.630        0.629       0.650       0.661   0.00970      *
    9        0.627        0.623       0.669       0.658   0.00961      *
   10        0.626        0.624       0.660       0.604   0.00951       
   11        0.637        0.637       0.627       0.683   0.00941       
   12        0.623        0.621       0.669       0.619   0.00932      *
   13        0.624        0.621       0.656       0.689   0.00923       
   14        0.616        0.615       0.667       0.644   0.00914      *
   15        0.618        0.618       0.665       0.629   0.00904       
   16        0.614        0.615       0.667       0.641   0.00895       
   17        0.617        0.617       0.662       0.608   0.00886       
   18        0.615        0.619       0.654       0.598   0.00878       
   19        0.616        0.615       0.659       0.683   0.00869      *
   20        0.614        0.618       0.654       0.661   0.00860       
   21        0.612        0.610       0.669       0.661   0.00851      *
   22        0.609        0.613       0.666       0.628   0.00843       
   23        0.610        0.611       0.670       0.629   0.00835       
   24        0.605        0.608       0.669       0.663   0.00826      *
   25        0.609        0.611       0.666       0.666   0.00818       
   26        0.611        0.615       0.669       0.623   0.00810       
   27        0.605        0.604       0.674       0.675   0.00802      *
   28        0.604        0.609       0.665       0.643   0.00794       
   29        0.612        0.610       0.662       0.680   0.00786       
   30        0.604        0.607       0.679       0.673   0.00778       
   31        0.603        0.604       0.672       0.659   0.00770       
   32        0.609        0.614       0.666       0.673   0.00762       
   33        0.604        0.605       0.670       0.646   0.00755       
   34        0.608        0.611       0.662       0.686   0.00747       
   35        0.602        0.606       0.667       0.638   0.00740       
   36        0.600        0.597       0.683       0.668   0.00732      *
   37        0.604        0.607       0.672       0.643   0.00725       
   38        0.605        0.607       0.676       0.645   0.00718       
   39        0.600        0.605       0.674       0.642   0.00711       
   40        0.602        0.607       0.674       0.648   0.00703       
   41        0.602        0.606       0.676       0.635   0.00696       
   42        0.603        0.606       0.663       0.617   0.00689       
   43        0.608        0.613       0.663       0.660   0.00683       
   44        0.610        0.611       0.659       0.582   0.00676       
   45        0.599        0.602       0.683       0.689   0.00669       
   46        0.601        0.603       0.676       0.640   0.00662       
   47        0.603        0.603       0.665       0.633   0.00656       
   48        0.599        0.604       0.674       0.639   0.00649       
   49        0.600        0.604       0.678       0.637   0.00643       
   50        0.600        0.605       0.671       0.640   0.00636       
   51        0.600        0.602       0.670       0.669   0.00630       
   52        0.597        0.602       0.679       0.680   0.00624       
   53        0.598        0.601       0.679       0.674   0.00617       
   54        0.599        0.602       0.678       0.678   0.00611       
   55        0.598        0.603       0.678       0.643   0.00605       
   56        0.601        0.608       0.672       0.614   0.00599       
--- Training ended ---
Number of model parameters: 2401
Training time: 0:02:36
Best epoch: 36
Best valid loss: 0.597
Best valid accuracy (macro): 0.683
Best F1 score: 0.668
2025-05-25 16:12:15   --- End running DenseMsaClassifier ---
2025-05-25 16:12:15   --- Start running DenseSiteClassifier ---
--- Hyperparameters ---
model = DenseSiteNet(
  (dense_layer1): Sequential(
    (0): Linear(in_features=91762, out_features=100, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=False)
  )
  (dense_layer2): Linear(in_features=100, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 9176401.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.693        0.693       0.485       0.360   0.00000      *
    2        0.040        0.087       0.971       0.972   0.00250      *
    3        0.018        0.063       0.978       0.979   0.00500      *
    4        0.011        0.063       0.982       0.982   0.00750      *
    5        0.023        0.122       0.974       0.974   0.01000       
    6        0.009        0.070       0.984       0.985   0.00990       
    7        0.007        0.062       0.985       0.985   0.00980      *
    8        0.004        0.053       0.989       0.989   0.00970      *
    9        0.001        0.044       0.991       0.991   0.00961      *
   10        0.002        0.058       0.985       0.985   0.00951       
   11        0.001        0.054       0.989       0.989   0.00941       
   12        0.001        0.058       0.989       0.989   0.00932       
   13        0.001        0.052       0.989       0.989   0.00923       
   14        0.005        0.066       0.985       0.985   0.00914       
   15        0.002        0.071       0.988       0.988   0.00904       
   16        0.001        0.062       0.988       0.988   0.00895       
   17        0.001        0.048       0.989       0.989   0.00886       
   18        0.001        0.048       0.990       0.990   0.00878       
   19        0.000        0.053       0.991       0.991   0.00869       
   20        0.002        0.047       0.989       0.989   0.00860       
   21        0.000        0.059       0.989       0.990   0.00851       
   22        0.001        0.053       0.988       0.988   0.00843       
   23        0.001        0.053       0.989       0.989   0.00835       
   24        0.000        0.051       0.989       0.990   0.00826       
   25        0.001        0.058       0.987       0.988   0.00818       
   26        0.001        0.053       0.990       0.990   0.00810       
   27        0.000        0.037       0.991       0.991   0.00802      *
   28        0.001        0.041       0.990       0.990   0.00794       
   29        0.001        0.048       0.991       0.991   0.00786       
   30        0.001        0.042       0.991       0.991   0.00778       
   31        0.000        0.038       0.992       0.993   0.00770       
   32        0.000        0.049       0.992       0.992   0.00762       
   33        0.000        0.029       0.992       0.993   0.00755      *
   34        0.001        0.035       0.990       0.990   0.00747       
   35        0.000        0.043       0.991       0.991   0.00740       
   36        0.000        0.036       0.990       0.990   0.00732       
   37        0.000        0.042       0.989       0.989   0.00725       
   38        0.000        0.040       0.988       0.989   0.00718       
   39        0.000        0.055       0.989       0.990   0.00711       
   40        0.000        0.049       0.989       0.989   0.00703       
   41        0.005        0.115       0.985       0.985   0.00696       
   42        0.000        0.060       0.989       0.989   0.00689       
   43        0.000        0.060       0.989       0.989   0.00683       
   44        0.000        0.061       0.989       0.989   0.00676       
   45        0.000        0.076       0.989       0.989   0.00669       
   46        0.000        0.055       0.989       0.989   0.00662       
   47        0.000        0.069       0.989       0.989   0.00656       
   48        0.000        0.061       0.989       0.989   0.00649       
   49        0.000        0.056       0.991       0.991   0.00643       
   50        0.001        0.052       0.989       0.989   0.00636       
   51        0.000        0.069       0.990       0.990   0.00630       
   52        0.000        0.050       0.990       0.990   0.00624       
   53        0.001        0.043       0.990       0.990   0.00617       
--- Training ended ---
Number of model parameters: 9176401
Training time: 0:08:16
Best epoch: 33
Best valid loss: 0.029
Best valid accuracy (macro): 0.992
Best F1 score: 0.993
2025-05-25 16:20:43   --- End running DenseSiteClassifier ---
2025-05-25 16:20:43   --- Start running AACnnClassifier ---
--- Hyperparameters ---
model = AAConvNet(
  (conv_layer): Sequential(
    (0): Conv1d(22, 64, kernel_size=(1,), stride=(1,))
    (1): ReLU()
    (2): AdaptiveAvgPool1d(output_size=1)
    (3): Dropout(p=0.2, inplace=False)
  )
  (dense_layer): Linear(in_features=64, out_features=1, bias=True)
)
split_proportion = 0.9
batch_pad_sequences = False
learning_rate = 0.01
batch_size = 64
max_epochs = 500
early_stopping_patience = 20
kernel_size = 1
--- Creating loaders ---
--- nvidia-smi output ---
nvidia-smi cannot be run: Command 'nvidia-smi' returned non-zero exit status 18.
--- Training start ---
Start training using cuda device.
Number of model parameters: 1537.
Epoch   Train loss   Valid loss   Valid acc    F1-score        Lr   Best
------------------------------------------------------------------------
    1        0.696        0.694       0.493       0.000   0.00000      *
    2        0.693        0.693       0.493       0.000   0.00250      *
    3        0.693        0.693       0.493       0.000   0.00500      *
    4        0.693        0.693       0.493       0.000   0.00750      *
    5        0.693        0.693       0.493       0.000   0.01000       
    6        0.693        0.694       0.493       0.000   0.00990       
    7        0.693        0.693       0.492       0.025   0.00980       
    8        0.693        0.693       0.493       0.000   0.00970       
    9        0.693        0.693       0.492       0.007   0.00961       
   10        0.693        0.693       0.493       0.000   0.00951       
   11        0.694        0.693       0.493       0.000   0.00941       
   12        0.694        0.694       0.493       0.000   0.00932       
   13        0.693        0.694       0.492       0.000   0.00923       
   14        0.693        0.693       0.488       0.462   0.00914       
   15        0.693        0.693       0.493       0.000   0.00904       
   16        0.693        0.694       0.493       0.000   0.00895       
   17        0.693        0.693       0.497       0.589   0.00886       
   18        0.694        0.694       0.493       0.000   0.00878       
   19        0.693        0.693       0.494       0.574   0.00869       
   20        0.693        0.693       0.497       0.036   0.00860       
   21        0.693        0.693       0.502       0.129   0.00851       
   22        0.693        0.693       0.492       0.152   0.00843       
   23        0.693        0.693       0.492       0.152   0.00835       
   24        0.693        0.693       0.497       0.164   0.00826       
--- Training ended ---
Number of model parameters: 1537
Training time: 0:04:56
Best epoch: 4
Best valid loss: 0.693
Best valid accuracy (macro): 0.493
Best F1 score: 0.000
2025-05-25 16:25:50   --- End running AACnnClassifier ---
